En este informe, analizaré el problema de calcular la distancia de edición entre dos cadenas de caracteres, utilizando operaciones con costos variables, como inserción, sustitución, transposición y eliminación, aplicadas una operación a la vez. Este análisis se realizará empleando dos enfoques algorítmicos distintos: Fuerza Bruta y Programación Dinámica. La elección de estos enfoques responde a la importancia de este problema en el campo del \textit{Análisis y Diseño de Algoritmos en Ciencias de la Computación}, ya que el cálculo de la distancia de edición ha permitido avances significativos en aplicaciones como la búsqueda de texto y en motores de búsqueda (por ejemplo, Google), donde es posible restringir resultados a palabras similares dentro de un margen específico de error.

Diversas investigaciones han explorado el concepto de búsqueda aproximada, en la cual se permite cierto grado de 'error' o diferencia entre dos cadenas de caracteres, una herramienta muy utilizada en áreas como la bioinformática. Un ejemplo relevante se menciona en \citetitle{MedigraphicPaper}, donde se destaca que \textit{"una de las principales herramientas que permiten la localización de características comunes en cadenas de proteínas o ADN de distintas especies es la búsqueda aproximada de cadenas."}. Esta técnica es fundamental para el análisis de secuencias biológicas, donde la distancia de edición se convierte en una métrica clave para medir similitudes y realizar comparaciones entre datos complejos.

El principal objetivo de este informe es resaltar la importancia de aplicar estos algoritmos de forma eficiente, reduciendo de manera significativa la complejidad temporal del problema, para ello se presentará una comparación en términos de velocidad entre los enfoques de Fuerza Bruta y Programación Dinámica. Además, se busca responder una de las preguntas fundamentales en el estudio de los algoritmos: \textbf{"¿Se puede hacer mejor?"}.

Con esta premisa en mente, el propósito de este informe es realizar un análisis exhaustivo del algoritmo de distancia de edición, centrándose principalmente en su análisis temporal. Para ello, se utilizarán diversas notaciones de complejidad, como la notación Big O, la más utilizada para evaluar el rendimiento de algoritmos. Este análisis permitirá explorar cómo un enfoque algorítmico distinto y más eficiente puede reducir significativamente el tiempo de ejecución, pasando de una complejidad exponencial a una lineal en ciertos casos. Además, se plantearán variaciones al problema estándar de distancia entre dos cadenas de carácteres, para evaluar si dichas variantes pueden ofrecer beneficios en aplicaciones prácticas actuales.